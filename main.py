# -*- coding: utf-8 -*-

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iSKhvHx6RDFoVw_rUQGj23bJS5QZMSNk
"""

from google.colab import drive

drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
#!pip install pandas
import joblib
import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.impute import SimpleImputer
!pip install category_encoders
import category_encoders as ce

# %matplotlib inline

sns.set(style = "darkgrid")

df = pd.read_csv("/content/drive/MyDrive/train.csv")

df.head()

df.dtypes

columns_to_drop = ['fire_origin','fire_name', 'discovered_size', 'industry_identifier_desc','fire_number','fire_year','distance_from_water_source','ex_fs_date','first_bucket_drop_date','fire_fighting_start_date','ia_arrival_at_fire_date','assessment_datetime','start_for_fire_date','dispatch_date','reported_date','discovered_date','fire_start_date','initial_action_by','wind_direction','fire_position_on_slope','assessment_resource','dispatched_resource','det_agent_type','det_agent','true_cause','activity_class','responsible_group_desc','bucketing_on_fire','fire_fighting_start_size','ia_access']
df.drop(columns=columns_to_drop, inplace=True)
columns_to_one_hot_encode = ["fire_type", "weather_conditions_over_fire", "general_cause_desc","fuel_type"]

# One-hot encode the specified columns
df_encoded = pd.get_dummies(df, columns=columns_to_one_hot_encode)

# Concatenate the one-hot encoded columns to the original DataFrame
df = pd.concat([df, df_encoded], axis=1)

# Drop the original columns that were one-hot encoded
df.drop(columns=columns_to_one_hot_encode, inplace=True)
duplicate_columns = df.columns[df.columns.duplicated()]

# Keep only unique columns
df = df.loc[:, ~df.columns.duplicated()]

print(df.columns)

column_names = df.columns.tolist()
print(column_names)

imputer = SimpleImputer(strategy='mean')
df_imputed = df.copy()
# selected_features = df_imputed.columns
# df_imputed[selected_features] = imputer.fit_transform(df_imputed[selected_features])


selected_features = df.columns.tolist()
selected_features.remove('size_class')

# Impute missing values for features only
imputer.fit(df_imputed[selected_features])
df_imputed[selected_features] = imputer.transform(df_imputed[selected_features])

# Separate features (X) and target variable (y)
X = df_imputed[selected_features]
y = df_imputed['size_class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)


y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
print('Classification Report:')
print(classification_report(y_test, y_pred))

model_filename = '/content/drive/MyDrive/Copy of Wildfire.ipynb.joblib'
loaded_model = joblib.load(model_filename)

# Load test data
test_data = pd.read_csv("/content/drive/MyDrive/test.csv")

# Select relevant features from test data and one-hot encode
columns_to_drop = ['fire_origin', 'fire_name', 'discovered_size', 'industry_identifier_desc', 'fire_number', 'fire_year', 'distance_from_water_source', 'ex_fs_date', 'first_bucket_drop_date', 'fire_fighting_start_date', 'ia_arrival_at_fire_date', 'assessment_datetime', 'start_for_fire_date', 'dispatch_date', 'reported_date', 'discovered_date', 'fire_start_date', 'initial_action_by', 'wind_direction', 'fire_position_on_slope', 'assessment_resource', 'dispatched_resource', 'det_agent_type', 'det_agent', 'true_cause', 'activity_class', 'responsible_group_desc', 'bucketing_on_fire', 'fire_fighting_start_size', 'ia_access']
columns_to_one_hot_encode = ["fire_type", "weather_conditions_over_fire", "general_cause_desc", "fuel_type"]

# One-hot encode the specified columns
test_data = pd.get_dummies(test_data.drop(columns=columns_to_drop), columns=columns_to_one_hot_encode)

# Ensure test data has all the expected features (fill missing columns with zeros)
expected_features = loaded_model.feature_names_in_
missing_columns = set(expected_features) - set(test_data.columns)
for col in missing_columns:
    test_data[col] = 0

# Reorder columns to match the expected order
test_data = test_data[expected_features]

# Impute missing values using mean imputation
imputer = SimpleImputer(strategy='mean')
test_data_imputed = pd.DataFrame(imputer.fit_transform(test_data), columns=test_data.columns)

# Predict size categories
predicted_size_categories = loaded_model.predict(test_data_imputed)

# Create a submission DataFrame
submission_df = pd.DataFrame({'fire_id': test_data['fire_id'], 'size_class': predicted_size_categories})

# Save the results to a CSV file
submission_filename = '/content/drive/MyDrive/submission_file_version2.csv'
submission_df.to_csv(submission_filename, index=False)

# Print unique predicted values and their counts
print("Unique predicted values:", set(predicted_size_categories))
print("Number of unique predicted values:", len(set(predicted_size_categories)))
